{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_gbq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-12e5644c0cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_gbq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_gbq'"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import pandas_gbq\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pandas-gbq --channel conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an environment variable to hold our google application credentials that are stored in the .json file\n",
    "#given by GBQ when we first made the database\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"C:/Users/jackc/OneDrive/Documents/MSBA/Applied Data Analytics/Wedge/cloughwedge2021-3249989ff592.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that contain necessary credentials to connect to our GBQ instance\n",
    "client = bigquery.Client(project = 'cloughwedge2021')\n",
    "gbq_proj_id = \"cloughwedge2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through our clean wedge files folder, put all of the names to a list\n",
    "path = os.getcwd()\n",
    "files = []\n",
    "\n",
    "for file_name in os.listdir(path + '\\\\Wedge_clean_files\\\\'):\n",
    "    files.append(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [12:43, 127.22s/it]\n",
      "7it [13:38, 116.89s/it]\n",
      "6it [13:15, 132.56s/it]\n",
      "c:\\users\\jackc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (43) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "6it [13:07, 131.31s/it]\n",
      "6it [14:32, 145.43s/it]\n",
      "3it [04:54, 98.11s/it]\n",
      "3it [04:50, 96.76s/it]\n",
      "2it [04:39, 139.84s/it]\n",
      "7it [14:15, 122.25s/it]\n",
      "7it [13:59, 119.86s/it]\n",
      "6it [13:44, 137.37s/it]\n",
      "1it [01:05, 65.82s/it]\n",
      "c:\\users\\jackc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "7it [12:55, 110.73s/it]\n",
      "1it [02:39, 159.86s/it]\n",
      "6it [13:23, 133.95s/it]\n",
      "1it [00:50, 50.50s/it]\n",
      "6it [13:36, 136.16s/it]\n",
      "1it [00:41, 41.51s/it]\n",
      "6it [12:59, 129.89s/it]\n",
      "1it [00:37, 37.40s/it]\n",
      "7it [13:39, 117.09s/it]\n",
      "1it [00:43, 43.09s/it]\n",
      "6it [14:25, 144.22s/it]\n",
      "1it [00:43, 43.90s/it]\n",
      "c:\\users\\jackc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (33,43) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "6it [12:06, 121.10s/it]\n",
      "1it [00:24, 24.38s/it]\n",
      "6it [13:26, 134.38s/it]\n",
      "1it [00:24, 24.61s/it]\n",
      "7it [13:50, 118.63s/it]\n",
      "1it [00:16, 16.77s/it]\n",
      "7it [12:29, 107.13s/it]\n",
      "1it [00:11, 11.88s/it]\n",
      "6it [11:59, 119.91s/it]\n",
      "1it [00:07,  7.13s/it]\n",
      "7it [12:49, 109.96s/it]\n",
      "7it [14:36, 125.22s/it]\n",
      "7it [14:25, 123.69s/it]\n",
      "3it [06:03, 121.14s/it]\n",
      "2it [06:25, 192.72s/it]\n",
      "2it [05:39, 169.79s/it]\n",
      "2it [07:46, 233.09s/it]\n",
      "2it [05:07, 153.69s/it]\n",
      "2it [04:08, 124.18s/it]\n",
      "2it [04:04, 122.16s/it]\n",
      "2it [04:06, 123.12s/it]\n",
      "2it [04:18, 129.23s/it]\n",
      "2it [04:21, 130.60s/it]\n",
      "2it [04:08, 124.20s/it]\n",
      "2it [04:15, 127.78s/it]\n",
      "2it [05:21, 160.94s/it]\n",
      "2it [04:25, 132.74s/it]\n",
      "2it [04:30, 135.16s/it]\n",
      "2it [04:05, 122.76s/it]\n"
     ]
    }
   ],
   "source": [
    "#The import.\n",
    "\n",
    "#Provide the project id and the database's name\n",
    "\n",
    "projID = \"cloughwedge2021.\"\n",
    "database = \"transactions\"\n",
    "\n",
    "#This will loop through each of the 50+ files in the wedge file folder and import each file's contents.\n",
    "\n",
    "#Using the Pandas package and its read_csv function, we create a data frame and fill it with the values in each csv file.\n",
    "#The data frame has the same variable names as they are listed in the csv files. If I were to do this part any different,\n",
    "#I would probably use lowercase letters in the column names. It was a massive pain in the ass to go through and make sure\n",
    "#that each time I used a variable or several variables in the SQL statements, the variable matched where I originally\n",
    "#claimed it. Obviously, I would miss a few then have to go back and see which ones were not capitalized like they should've\n",
    "#been. ANYWAYS...\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    TableName = projID + file[:-4]\n",
    "    df = pd.read_csv(path + '\\\\Wedge_clean_files\\\\' + file, names =['DateTime','Register_Num','Emp_Num',\"Trans_Num\", \"UPC\",\"Description\",'Trans_Type',\"Trans_SubType\",'Trans_Status',\n",
    "                                    'Department','Quantity','Scale','Cost','Unit_Price','Total','Reg_Price','Alt_Price','Tax',\n",
    "                                    \"TaxExempt\",'Foodstamp','WicAble','Discount','memDiscount','Discountable','DiscountType',\n",
    "                                    'Voided','PercentDiscount','ItemQtty','volDiscType','Volume','VolSpecial','mixMatch','Matched'\n",
    "                                    ,'MemType','Staff','NumFlag','itemStatus','TenderStatus','CharFlag','varflag','BatchHeaderID','Local',\n",
    "                                    'Organic','Display','Receipt','Card_No','Store','Branch','Match_ID','Trans_ID'])\n",
    "    \n",
    "    #Here's what solves an error I and a classmate were having, where the import was giving a warning message about\n",
    "    #the types of varaibles that were being imported. This is solved by converting the variables in question to\n",
    "    #boolean type, which seems to have solved the issue.\n",
    "    \n",
    "    cols_to_bool = [\"MemType\", \"Staff\", \"BatchHeaderID\", \"Display\"]\n",
    "    df[cols_to_bool] = df[cols_to_bool].astype('bool')\n",
    "\n",
    "    #Here's where the datagrame gets sent to our GBQ instance, and here's also where that capitalization\n",
    "    #\"pain in the ass\" thing comes in. Essentially, we're gonna assign a schema to the dataframe, enlarge the chunk\n",
    "    #size to 500000 so that it works faster and GBQ doesn't get overrun with how many requests were making to it,\n",
    "    #and ensuring that the import will override a table if it already exists (RIP after having to do this\n",
    "    #multiple times because it took FOREVER. Also, the table schema calls requires asssigning variable types to\n",
    "    #each column. This needs to be perfectly in line with where we gave each variable a type when we created\n",
    "    #the data frame.\n",
    "    \n",
    "    DataFrame.to_gbq(df,TableName,gbq_proj_id,chunksize = 500000, if_exists = \"replace\",\n",
    "                    table_schema = [{'name': 'DateTime', 'type': 'Timestamp'}, {'name': 'Register_Num', 'type': 'Float'},\n",
    "{'name': 'Emp_Num', 'type': 'Float'}, {'name': 'Trans_Num', 'type': 'Float'},\n",
    "{'name': 'UPC', 'type': 'string'}, {'name': 'Description', 'type': 'string'},\n",
    "{'name': 'Trans_Type', 'type': 'string'}, {'name': 'Trans_SubType', 'type': 'string'},\n",
    "{'name': 'Trans_Status', 'type': 'string'}, {'name': 'Department', 'type': 'Float'},\n",
    "{'name': 'Quantity', 'type': 'float'}, {'name': 'Scale', 'type': 'Float'},\n",
    "{'name': 'Cost', 'type': 'float'}, {'name': 'Unit_Price', 'type': 'float'},\n",
    "{'name': 'Total', 'type': 'float'}, {'name': 'Reg_Price', 'type': 'float'},\n",
    "{'name': 'Alt_Price', 'type': 'float'}, {'name': 'Tax', 'type': 'float'},\n",
    "{'name': 'TaxExempt', 'type': 'float'}, {'name': 'Foodstamp', 'type': 'Float'},\n",
    "{'name': 'WicAble', 'type': 'Float'}, {'name': 'Discount', 'type': 'float'},\n",
    "{'name': 'memDiscount', 'type': 'float'}, {'name': 'Discountable', 'type': 'float'},\n",
    "{'name': 'DiscountType', 'type': 'float'}, {'name': 'Voided', 'type': 'float'},\n",
    "{'name': 'PercentDiscount', 'type': 'float'}, {'name': 'ItemQtty', 'type': 'float'},\n",
    "{'name': 'volDiscType', 'type': 'float'}, {'name': 'Volume', 'type': 'float'},\n",
    "{'name': 'VolSpecial', 'type': 'float'}, {'name': 'mixMatch', 'type': 'float'},\n",
    "{'name': 'Matched', 'type': 'float'}, {'name': 'MemType', 'type': 'Boolean'},\n",
    "{'name': 'Staff', 'type': 'Boolean'}, {'name': 'NumFlag', 'type': 'float'},\n",
    "{'name': 'itemStatus', 'type': 'float'}, {'name': 'TenderStatus', 'type': 'float'},\n",
    "{'name': 'CharFlag', 'type': 'string'}, {'name': 'varFlag', 'type': 'float'},\n",
    "{'name': 'BatchHeaderID', 'type': 'boolean'}, {'name': 'Local', 'type': 'float'},\n",
    "{'name': 'Organic', 'type': 'float'}, {'name': 'Display', 'type': 'Boolean'},\n",
    "{'name': 'Receipt', 'type': 'float'}, {'name': 'Card_No', 'type': 'float'},\n",
    "{'name': 'Store', 'type': 'float'}, {'name': 'Branch', 'type': 'float'},\n",
    "{'name': 'Match_ID', 'type': 'float'}, {'name': 'Trans_ID', 'type': 'float'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.05s: 100%|████████████████████████████████████████████████████| 2/2 [00:00<00:00, 509.23query/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.41s/rows]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85760139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f0_\n",
       "0  85760139"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here I did a count of how many records we have in the GBQ isntance to make sure we got everything in, which it seems like\n",
    "#we did! Also, I wanted to find a different way of using the SQL commands in python other than having them in comment\n",
    "#format and assigning that to a variable to be called later, so I foud this. I didn't use it later on, but it still\n",
    "#looks nicer.\n",
    "\n",
    "%%bigquery\n",
    "SELECT\n",
    "    COUNT(UPC) \n",
    "FROM `cloughwedge2021.cloughwedge2021.transArchive*`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
